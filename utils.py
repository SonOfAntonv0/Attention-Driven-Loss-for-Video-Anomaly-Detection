import matplotlib
matplotlib.use('Agg')
import tensorflow as tf
import numpy as np
from collections import OrderedDict
import os
import glob
import cv2
import matplotlib.pyplot as plt

rng = np.random.RandomState(2017)


def np_load_frame(filename, resize_height, resize_width):
    """
    Load image path and convert it to numpy.ndarray. Notes that the color channels are BGR and the color space
    is normalized from [0, 255] to [-1, 1].

    :param filename: the full path of image
    :param resize_height: resized height
    :param resize_width: resized width
    :return: numpy.ndarray
    """
    image_decoded = cv2.imread(filename)
    image_resized = cv2.resize(image_decoded, (resize_width, resize_height))
    image_resized = image_resized.astype(dtype=np.float32)
    image_resized = (image_resized / 127.5) - 1.0
    return image_resized


class DataLoader(object):
    def __init__(self, video_folder, resize_height=256, resize_width=256):
        self.dir = video_folder
        self.videos = OrderedDict()
        self._resize_height = resize_height
        self._resize_width = resize_width
        self.setup()

    def __call__(self, batch_size, time_steps, num_pred=1):
        video_info_list = list(self.videos.values())
        num_videos = len(video_info_list)

        clip_length = time_steps + num_pred
        resize_height, resize_width = self._resize_height, self._resize_width

        def video_clip_generator():
            v_id = -1
            while True:
                v_id = (v_id + 1) % num_videos
             
                video_info = video_info_list[v_id]
                start = rng.randint(0, video_info['length'] - clip_length)
                video_clip = []
                for frame_id in range(start, start + clip_length):
                    video_clip.append(np_load_frame(video_info['frame'][frame_id], resize_height, resize_width))
                video_clip = np.concatenate(video_clip, axis=2)

                yield video_clip

        # video clip paths
        dataset = tf.data.Dataset.from_generator(generator=video_clip_generator,
                                                 output_types=tf.float32,
                                                 output_shapes=[resize_height, resize_width, clip_length * 3])
        print('generator dataset, {}'.format(dataset))
        dataset = dataset.prefetch(buffer_size=1000)
        dataset = dataset.shuffle(buffer_size=1000).batch(batch_size)
        print('epoch dataset, {}'.format(dataset))

        return dataset

    def __getitem__(self, video_name):
        assert video_name in self.videos.keys(), 'video = {} is not in {}!'.format(video_name, self.videos.keys())
        return self.videos[video_name]

    def setup(self):
        videos = glob.glob(os.path.join(self.dir, '*'))
        for video in sorted(videos):
            video_name = video.split('/')[-1]
            self.videos[video_name] = {}
            self.videos[video_name]['path'] = video
            self.videos[video_name]['frame'] = glob.glob(os.path.join(video, '*.jpg'))
            self.videos[video_name]['frame'].sort()
            self.videos[video_name]['length'] = len(self.videos[video_name]['frame'])

    def get_video_clips(self, video, start, end):
        # assert video in self.videos, 'video = {} must in {}!'.format(video, self.videos.keys())
        # assert start >= 0, 'start = {} must >=0!'.format(start)
        # assert end <= self.videos[video]['length'], 'end = {} must <= {}'.format(video, self.videos[video]['length'])

        batch = []
        for i in range(start, end):
            image = np_load_frame(self.videos[video]['frame'][i], self._resize_height, self._resize_width)
            batch.append(image)

        return np.concatenate(batch, axis=2)


def log10(t):
    """
    Calculates the base-10 log of each element in t.

    @param t: The tensor from which to calculate the base-10 log.

    @return: A tensor with the base-10 log of each element in t.
    """

    numerator = tf.math.log(t)
    denominator = tf.math.log(tf.constant(10, dtype=numerator.dtype))
    return numerator / denominator


def psnr_error(gen_frames, gt_frames):
    """
    Computes the Peak Signal to Noise Ratio error between the generated images and the ground
    truth images.

    @param gen_frames: A tensor of shape [batch_size, height, width, 3]. The frames generated by the
                       generator model.
    @param gt_frames: A tensor of shape [batch_size, height, width, 3]. The ground-truth frames for
                      each frame in gen_frames.

    @return: A scalar tensor. The mean Peak Signal to Noise Ratio error over each frame in the
             batch.
    """
    shape = tf.shape(gen_frames)
    num_pixels = tf.cast(shape[1] * shape[2] * shape[3],dtype=tf.float32)#to_float
    gt_frames = (gt_frames + 1.0) / 2.0
    gen_frames = (gen_frames + 1.0) / 2.0

    square_diff = tf.square(gt_frames - gen_frames)

    # batch_errors = 10 * log10(1 / (square_diff))
    # return batch_errors
    batch_errors = 10 * log10(1 / ((1 / num_pixels) * tf.reduce_sum(square_diff, [1, 2, 3])))
    return tf.reduce_mean(batch_errors)

def diff_gt(gen_frames,gt_frames):

    square_diff = tf.abs(gt_frames - gen_frames)
    log_diff = tf.math.log(square_diff)
    return square_diff,log_diff

def psnr_error_objectness(gen_frames, gt_frames, objectness_attention):
    """
    Computes the Peak Signal to Noise Ratio error between the generated images and the ground
    truth images.

    @param gen_frames: A tensor of shape [batch_size, height, width, 3]. The frames generated by the
                       generator model.
    @param gt_frames: A tensor of shape [batch_size, height, width, 3]. The ground-truth frames for
                      each frame in gen_frames.

    @return: A scalar tensor. The mean Peak Signal to Noise Ratio error over each frame in the
             batch.
    """
    shape = tf.shape(gen_frames)
    num_pixels = tf.cast(shape[1] * shape[2] * shape[3],dtype=tf.float32)
    gt_frames = (gt_frames + 1.0) / 2.0
    gen_frames = (gen_frames + 1.0) / 2.0
    stack_frames = tf.stack([gt_frames, gen_frames], axis=3)
    stack_frames_min = tf.math.reduce_min(stack_frames, 3)
    stack_frames_max = tf.math.reduce_max(stack_frames, 3)

    reg_term = 1 - stack_frames_min / stack_frames_max

    square_diff = tf.square(gt_frames - gen_frames) * (objectness_attention + reg_term)

    batch_errors = 10 * log10(1 / ((1 / num_pixels) * tf.reduce_sum(square_diff, [1, 2, 3])))
    return tf.reduce_mean(batch_errors)


def diff_mask(gen_frames, gt_frames, min_value=-1, max_value=1):
    # normalize to [0, 1]
    delta = max_value - min_value
    gen_frames = (gen_frames - min_value) / delta
    gt_frames = (gt_frames - min_value) / delta

    gen_gray_frames = tf.image.rgb_to_grayscale(gen_frames)
    gt_gray_frames = tf.image.rgb_to_grayscale(gt_frames)

    diff = tf.abs(gen_gray_frames - gt_gray_frames)
    return diff


def load(saver, sess, ckpt_path):
    saver.restore(sess, ckpt_path)
    print("Restored model parameters from {}".format(ckpt_path))


def save(saver, sess, logdir, step):
    model_name = 'model.ckpt'
    checkpoint_path = os.path.join(logdir, model_name)
    if not os.path.exists(logdir):
        os.makedirs(logdir)
    saver.save(sess, checkpoint_path, global_step=step)
    print('The checkpoint has been created.')


def dynamic_image(X):
    X = np.swapaxes(X,0,-1)
    im_shape=np.shape(X)    
    N=im_shape[2]

    fw=np.zeros(shape=(1,1,N))
    for x_index in range(N):
        fw[0,0,x_index]=2*(x_index+1)-N-1

    # fw = np.sign(fw)
    Y=np.multiply(X,fw)

    DI=np.zeros(shape=(im_shape[0],im_shape[1]))  
    for y_index in range(N):
        tmp=Y[:,:,y_index]
        DI=DI+tmp

    pixel_max=np.max(DI)
    pixel_min=np.min(DI)
    DI_gray =np.abs(DI)/pixel_max
    DI_gray = np.swapaxes(DI_gray,0,1)
    DI = DI_gray
 
    pixel_max=np.max(DI)
    pixel_min=np.min(DI)

    DI_gray=(DI-pixel_min)/(pixel_max-pixel_min)
    DI_bias = np.ones_like(DI_gray).astype(np.float32) * 0.5
    DI_gray += DI_bias


    return DI_gray * 3

def objectness_rgb_estimation(gt_frames):

    width = gt_frames.shape[2]
    height = gt_frames.shape[1]
    '''
    images = tf.Session().run(gt_frames)
    objectness_map =np.zeros_like(images)
    for i in range(images.shape[3]):
        image = images[:,:,:,i]
        
        di = dynamic_image(image)
        objectness_map[:,:,:,i] = di
    '''
    objectness_map = np.load("C:/Users/Venky/Documents/Jupyter/Some work v1.2/Mini Project 7th sem/Attention-Driven Loss for Anomaly Detection in Video Surveillance/Codes/Results/di.npy")
    objectness_map = objectness_map[:4,:,:,:]
    objectness_map = np.abs(objectness_map)
    #all 1 attention map
    # objectness_map = np.ones_like(objectness_map)
    objectness_map = objectness_map.astype(np.float32) / 0.75
    obj_map_tensor = tf.convert_to_tensor(objectness_map)

    return obj_map_tensor

def universal_di(gt_frames):

    width = gt_frames.shape[2]
    height = gt_frames.shape[1]

    images = gt_frames
    objectness_map =np.zeros_like(images).astype(np.float64)
    for i in range(images.shape[3]):
        image = images[:,:,:,i]
        
        di = dynamic_image(image)
        objectness_map[:,:,:,i] = di

    return objectness_map

def imgshow(img,savename):
    if img.shape[2]!=3:
        img = np.repeat(img,3,axis=2)
        img = np.abs(img)
    img = img / 5
    from matplotlib import pyplot as plt
    plt.figure(figsize=(15,8))
    plt.imshow(img)
    plt.savefig(savename+".jpg",format="jpg")


def get_universal_di():
    path = "C:/Users/Venky/Documents/Jupyter/Some work v1.2/Mini Project 7th sem/Attention-Driven Loss for Anomaly Detection in Video Surveillance/Data/ped2/training/frames"
    import os 
    import cv2 
    import os.path as osp
    from glob import glob
    images = []
    path_list = os.listdir(path)
    path_list.sort()
    count = 0
    for each in path_list:
        path1 = osp.join(path,each)
        iters = glob(osp.join(path1,"*.jpg"))
        iters.sort()
        for img in iters:
            print(img)
            images.append(cv2.resize(cv2.imread(img),(256,256)))
            count += 1 
            if count >= 2000:
                break
    np.save("C:/Users/Venky/Documents/Jupyter/Some work v1.2/Mini Project 7th sem/Attention-Driven Loss for Anomaly Detection in Video Surveillance/Codes/Results/images.npy",images)
    di = universal_di(np.array(images))
    imgshow(di[0],"di_0")
    np.save("di",di)
if __name__ == '__main__':

    get_universal_di()

